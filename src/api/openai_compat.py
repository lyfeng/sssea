"""
OpenAI Chat Completion Compatible API

å®ç°ç¬¦åˆ OpenAI API æ ‡å‡†çš„æ¥å£ï¼Œä½¿ SSSEA èƒ½è¢«å…¶ä»– Agent é€šè¿‡æ ‡å‡† SDK è°ƒç”¨ã€‚
æ”¯æŒROMA Pipelineè¿›è¡Œå®Œæ•´çš„é€’å½’æ¨ç†åˆ†æã€‚
"""

import json
import logging
import time
import uuid
from datetime import datetime, timezone
from typing import Any, Dict, List, Optional

from fastapi import HTTPException
from pydantic import BaseModel, Field

from ..simulation.models import SimulationRequest, SimulationResult
from ..reasoning import (
    MockIntentAnalyzer,
    IntentAnalyzer,
    ROMAIntentAnalyzer,
    MockROMAAnalyzer,
)
from ..attestation.mock_quote import generate_attestation_metadata
from ..config import get_settings


logger = logging.getLogger(__name__)


# =============================================================================
# OpenAI API Request/Response Models
# =============================================================================


class ChatMessage(BaseModel):
    """Chat æ¶ˆæ¯"""
    role: str
    content: str
    name: Optional[str] = None
    tool_calls: Optional[List[Any]] = None


class ToolFunction(BaseModel):
    """Tool å‡½æ•°å®šä¹‰"""
    name: str
    arguments: str  # JSON string


class ToolCall(BaseModel):
    """Tool è°ƒç”¨"""
    id: str
    type: str = "function"
    function: ToolFunction


class Tool(BaseModel):
    """Tool å®šä¹‰"""
    type: str = "function"
    function: Dict[str, Any]


class ToolChoice(BaseModel):
    """Tool é€‰æ‹©"""
    type: str = "function"
    function: Dict[str, str]


class ChatCompletionRequest(BaseModel):
    """Chat Completion è¯·æ±‚"""
    model: str
    messages: List[ChatMessage]
    tools: Optional[List[Tool]] = None
    tool_choice: Optional[str | ToolChoice] = "auto"
    temperature: Optional[float] = 0.7
    max_tokens: Optional[int] = None
    stream: Optional[bool] = False


class Usage(BaseModel):
    """Token ä½¿ç”¨ç»Ÿè®¡"""
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int


class ChatCompletionResponse(BaseModel):
    """Chat Completion å“åº”"""
    id: str
    object: str = "chat.completion"
    created: int
    model: str
    choices: List[Dict[str, Any]]
    usage: Usage
    system_fingerprint: Optional[str] = None

    # SSSEA æ‰©å±•å­—æ®µ
    metadata: Optional[Dict[str, Any]] = None


class SimulationToolArgs(BaseModel):
    """simulate_tx å·¥å…·å‚æ•°"""
    user_intent: str
    chain_id: int = 1
    tx_from: str
    tx_to: str
    tx_value: str = "0"
    tx_data: str = "0x"


# =============================================================================
# SSSEA Tool Definitions
# =============================================================================

SIMULATE_TX_TOOL = {
    "type": "function",
    "function": {
        "name": "simulate_tx",
        "description": (
            "åœ¨ TEE éš”ç¦»æ²™ç›’ä¸­æ¨¡æ‹Ÿ Web3 äº¤æ˜“æ‰§è¡Œï¼Œå¹¶è¿›è¡Œæ„å›¾å¯¹é½å®¡è®¡ã€‚"
            "è¿”å›è¯¦ç»†çš„èµ„äº§å˜åŠ¨ã€é£é™©è¯„çº§å’Œ OML è¯æ˜ã€‚"
            "åŸºäºROMAæ¡†æ¶è¿›è¡Œé€’å½’æ¨ç†åˆ†æã€‚"
        ),
        "parameters": {
            "type": "object",
            "properties": {
                "user_intent": {
                    "type": "string",
                    "description": "ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€æ„å›¾ï¼Œå¦‚ 'Swap 1 ETH to USDC, slippage 0.5%'",
                },
                "chain_id": {
                    "type": "integer",
                    "description": "é“¾ IDï¼Œé»˜è®¤ä¸ºä»¥å¤ªåŠä¸»ç½‘ (1)",
                    "default": 1,
                },
                "tx_from": {
                    "type": "string",
                    "description": "äº¤æ˜“å‘èµ·è€…åœ°å€",
                },
                "tx_to": {
                    "type": "string",
                    "description": "äº¤æ˜“ç›®æ ‡åœ°å€",
                },
                "tx_value": {
                    "type": "string",
                    "description": "äº¤æ˜“ valueï¼ˆwei æ ¼å¼ï¼‰",
                    "default": "0",
                },
                "tx_data": {
                    "type": "string",
                    "description": "äº¤æ˜“ calldata",
                    "default": "0x",
                },
            },
            "required": ["user_intent", "tx_from", "tx_to"],
        },
    },
}


# =============================================================================
# API Handler
# =============================================================================

class SSSEAHandler:
    """
    SSSEA API å¤„ç†å™¨

    å¤„ç† OpenAI å…¼å®¹çš„ Chat Completion è¯·æ±‚ã€‚
    æ”¯æŒé€šè¿‡ROMA Pipelineè¿›è¡Œå®Œæ•´çš„é€’å½’æ¨ç†åˆ†æã€‚
    """

    def __init__(self, settings: Optional[Any] = None):
        self.settings = settings or get_settings()
        self.analyzer = self._create_analyzer()
        self._roma_pipeline = None
        self._screener = None

    def _create_analyzer(self) -> Any:
        """
        æ ¹æ®é…ç½®åˆ›å»ºæ¨ç†åˆ†æå™¨

        æ”¯æŒçš„æ¨ç†å¼•æ“ï¼š
        - "roma_pipeline": ä½¿ç”¨å®Œæ•´çš„ROMA Pipelineï¼ˆæ¨èï¼‰
        - "roma": ä½¿ç”¨ ROMA é€’å½’æ¨ç†æ¡†æ¶ï¼ˆæ—§ç‰ˆï¼‰
        - "openai": ç›´æ¥ä½¿ç”¨ OpenAI API
        - "mock": ä½¿ç”¨ Mock åˆ†æå™¨ï¼ˆæµ‹è¯•ç”¨ï¼‰
        """
        engine = self.settings.reasoning_engine.lower()

        # ä¼˜å…ˆä½¿ç”¨ROMA Pipeline
        if engine in ("roma", "roma_pipeline"):
            api_key = self.settings.roma_api_key or self.settings.openai_api_key
            if api_key:
                logger.info("Using ROMA Pipeline for analysis")
                # å°è¯•åˆå§‹åŒ–ROMA Pipeline
                try:
                    from ..agents import SSSEAPipeline
                    from ..config.roma_config import load_profile

                    config = load_profile("dev" if self.settings.api_reload else "prod")
                    self._roma_pipeline = SSSEAPipeline(config)
                    return None  # ä½¿ç”¨Pipelineæ›¿ä»£analyzer
                except ImportError as e:
                    logger.warning(f"ROMA Pipelineä¸å¯ç”¨: {e}, å›é€€åˆ°ROMAIntentAnalyzer")
                    return ROMAIntentAnalyzer(
                        api_key=api_key,
                        base_url=self.settings.openai_base_url,
                        model=self.settings.roma_model,
                        provider=self.settings.roma_provider,
                    )
            else:
                logger.warning("ROMA API key not configured, using MockROMAAnalyzer")
                return MockROMAAnalyzer()

        elif engine == "openai":
            # ä½¿ç”¨ä¼ ç»Ÿ OpenAI
            if self.settings.openai_api_key:
                logger.info(f"Using IntentAnalyzer with model: {self.settings.openai_model}")
                return IntentAnalyzer(
                    api_key=self.settings.openai_api_key,
                    base_url=self.settings.openai_base_url,
                    model=self.settings.openai_model,
                )
            else:
                logger.warning("OpenAI API key not configured, using MockIntentAnalyzer")
                return MockIntentAnalyzer()

        else:  # engine == "mock" or unknown
            logger.info("Using MockROMAAnalyzer for testing")
            return MockROMAAnalyzer()

    async def handle_chat_completion(
        self,
        request: ChatCompletionRequest,
    ) -> ChatCompletionResponse:
        """
        å¤„ç† Chat Completion è¯·æ±‚

        Args:
            request: Chat Completion è¯·æ±‚

        Returns:
            ChatCompletionResponse: å“åº”
        """
        # æ£€æŸ¥æ˜¯å¦è¯·æ±‚äº† simulate_tx å·¥å…·
        if request.tools and any(
            t.function.get("name") == "simulate_tx"
            for t in request.tools
        ):
            return await self._handle_simulation(request)

        # å¦‚æœæ²¡æœ‰è¯·æ±‚å·¥å…·ï¼Œè¿”å›æ™®é€šèŠå¤©å“åº”
        return await self._handle_chat(request)

    async def _handle_simulation(
        self,
        request: ChatCompletionRequest,
    ) -> ChatCompletionResponse:
        """
        å¤„ç†æ¨¡æ‹Ÿè¯·æ±‚

        æ ¹æ®é…ç½®é€‰æ‹©æ‰§è¡Œè·¯å¾„ï¼š
        1. ROMA Pipeline: å®Œæ•´çš„é€’å½’æ¨ç†åˆ†æ
        2. ROMA Intent Analyzer: ä½¿ç”¨ROMAæ¡†æ¶çš„åˆ†æ
        3. ä¼ ç»Ÿåˆ†æ: ç›´æ¥è°ƒç”¨æ¨¡æ‹Ÿ+åˆ†æ
        """
        # 1. æå–æ„å›¾å’Œäº¤æ˜“æ•°æ®
        intent, tx_params = self._extract_transaction_params(request)

        # 2. å¦‚æœæœ‰ROMA Pipelineï¼Œä½¿ç”¨Pipeline
        if self._roma_pipeline:
            return await self._handle_with_pipeline(request, intent, tx_params)

        # 3. å¦åˆ™ä½¿ç”¨ä¼ ç»Ÿåˆ†ææµç¨‹
        return await self._handle_with_analyzer(request, intent, tx_params)

    async def _handle_with_pipeline(
        self,
        request: ChatCompletionRequest,
        intent: str,
        tx_params: Dict[str, Any],
    ) -> ChatCompletionResponse:
        """ä½¿ç”¨ROMA Pipelineå¤„ç†è¯·æ±‚"""
        try:
            # è¿è¡ŒROMA Pipeline
            result = await self._roma_pipeline.run(
                user_intent=intent,
                tx_data=tx_params,
            )

            # æ„å»ºå“åº”
            return self._build_pipeline_response(request, intent, tx_params, result)

        except Exception as e:
            logger.error(f"ROMA Pipelineæ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
            # å›é€€åˆ°ä¼ ç»Ÿåˆ†æ
            return await self._handle_with_analyzer(request, intent, tx_params)

    def _build_pipeline_response(
        self,
        request: ChatCompletionRequest,
        intent: str,
        tx_params: Dict[str, Any],
        result: Dict[str, Any],
    ) -> ChatCompletionResponse:
        """æ„å»ºPipelineå“åº”"""
        verdict = result.get("verdict", {})
        tool_call_id = f"call_{uuid.uuid4().hex[:24]}"

        # æ ¼å¼åŒ–å“åº”æ¶ˆæ¯
        content = self._format_pipeline_message(result)

        # ç”Ÿæˆè¯æ˜
        attestation = generate_attestation_metadata(
            simulation_result={
                "risk_level": verdict.get("risk_level", "UNKNOWN"),
                "confidence": verdict.get("confidence", 0.7),
            },
            model_name=request.model,
        )

        return ChatCompletionResponse(
            id=f"chatcmpl-{uuid.uuid4().hex[:28]}",
            created=int(time.time()),
            model=request.model,
            choices=[{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": content,
                    "tool_calls": [{
                        "id": tool_call_id,
                        "type": "function",
                        "function": {
                            "name": "simulate_tx",
                            "arguments": json.dumps(tx_params),
                        },
                    }],
                },
                "finish_reason": "tool_calls",
            }],
            usage=Usage(
                prompt_tokens=100,
                completion_tokens=len(content) // 4,
                total_tokens=100 + len(content) // 4,
            ),
            system_fingerprint=attestation["system_fingerprint"],
            metadata={
                "oml_attestation": attestation["oml_attestation"],
                "risk_level": verdict.get("risk_level", "UNKNOWN"),
                "risk_score": int(verdict.get("confidence", 0.7) * 100),
                "pipeline_used": True,
                "execution_steps": result.get("execution_details", {}).get("steps", []),
            },
        )

    def _format_pipeline_message(self, result: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–Pipelineç»“æœæ¶ˆæ¯"""
        verdict = result.get("verdict", {})
        risk_level = verdict.get("risk_level", "UNKNOWN")

        risk_emoji = {
            "SAFE": "âœ…",
            "WARNING": "âš ï¸",
            "CRITICAL": "ğŸš¨",
        }

        emoji = risk_emoji.get(risk_level, "")
        lines = [
            f"{emoji} **å®‰å…¨å®¡è®¡ç»“æœ**: {risk_level}",
            f"**ç½®ä¿¡åº¦**: {verdict.get('confidence', 0.7):.0%}",
            "",
            f"**æ‘˜è¦**: {result.get('summary', '')}",
        ]

        findings = result.get("findings", [])
        if findings:
            lines.extend(["", **æ£€æµ‹åˆ°çš„é—®é¢˜**:])
            lines.extend(f"- {f}" for f in findings)

        recommendations = result.get("recommendations", [])
        if recommendations:
            lines.extend(["", **å»ºè®®**:])
            lines.extend(f"- {r}" for r in recommendations[:5])

        return "\n".join(lines)

    async def _handle_with_analyzer(
        self,
        request: ChatCompletionRequest,
        intent: str,
        tx_params: Dict[str, Any],
    ) -> ChatCompletionResponse:
        """ä½¿ç”¨ä¼ ç»ŸAnalyzerå¤„ç†è¯·æ±‚"""
        # æ„å»ºæ¨¡æ‹Ÿè¯·æ±‚
        sim_request = SimulationRequest(
            user_intent=intent,
            chain_id=tx_params.get("chain_id", 1),
            tx_from=tx_params["tx_from"],
            tx_to=tx_params["tx_to"],
            tx_value=tx_params.get("tx_value", "0"),
            tx_data=tx_params.get("tx_data", "0x"),
        )

        # æ‰§è¡Œæ¨¡æ‹Ÿ
        sim_result = await self._run_simulation(sim_request)

        # æ„å›¾åˆ†æ
        analysis = await self.analyzer.analyze(sim_request, sim_result)

        # ç”Ÿæˆè¯æ˜
        attestation = generate_attestation_metadata(
            simulation_result={
                "risk_level": analysis.risk_level.value,
                "confidence": analysis.confidence,
                "anomalies": analysis.anomalies,
            },
            model_name=request.model,
        )

        tool_call_id = f"call_{uuid.uuid4().hex[:24]}"

        return ChatCompletionResponse(
            id=f"chatcmpl-{uuid.uuid4().hex[:28]}",
            created=int(time.time()),
            model=request.model,
            choices=[{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": self._format_response_message(analysis, sim_result),
                    "tool_calls": [{
                        "id": tool_call_id,
                        "type": "function",
                        "function": {
                            "name": "simulate_tx",
                            "arguments": json.dumps(tx_params),
                        },
                    }],
                },
                "finish_reason": "tool_calls",
            }],
            usage=Usage(
                prompt_tokens=100,
                completion_tokens=len(analysis.analysis) // 4,
                total_tokens=100 + len(analysis.analysis) // 4,
            ),
            system_fingerprint=attestation["system_fingerprint"],
            metadata={
                "oml_attestation": attestation["oml_attestation"],
                "risk_level": analysis.risk_level.value,
                "risk_score": int(analysis.confidence * 100),
                "pipeline_used": False,
                "asset_impact": {
                    c.token_symbol: c.change_amount
                    for c in sim_result.asset_changes
                },
            },
        )

    async def _handle_chat(
        self,
        request: ChatCompletionRequest,
    ) -> ChatCompletionResponse:
        """å¤„ç†æ™®é€šèŠå¤©è¯·æ±‚"""
        last_message = request.messages[-1].content if request.messages else ""

        response = ChatCompletionResponse(
            id=f"chatcmpl-{uuid.uuid4().hex[:28]}",
            created=int(time.time()),
            model=request.model,
            choices=[{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": (
                        "æˆ‘æ˜¯ SSSEA å®‰å…¨å®¡è®¡ Agentï¼ŒåŸºäº ROMA æ¡†æ¶è¿›è¡Œé€’å½’æ¨ç†åˆ†æã€‚"
                        "è¯·ä½¿ç”¨ simulate_tx å·¥å…·æ¥å®¡è®¡ Web3 äº¤æ˜“ã€‚"
                    ),
                },
                "finish_reason": "stop",
            }],
            usage=Usage(
                prompt_tokens=10,
                completion_tokens=25,
                total_tokens=35,
            ),
            system_fingerprint=f"sssea-roma@{uuid.uuid4().hex[:8]}",
        )

        return response

    def _extract_transaction_params(
        self,
        request: ChatCompletionRequest,
    ) -> tuple[str, Dict[str, Any]]:
        """
        ä»è¯·æ±‚ä¸­æå–äº¤æ˜“å‚æ•°

        ä¼˜å…ˆçº§ï¼š
        1. tool_calls ä¸­çš„å‚æ•°
        2. ç”¨æˆ·æ¶ˆæ¯ä¸­çš„ JSON
        3. æ¶ˆæ¯æ–‡æœ¬è§£æ
        """
        # æ£€æŸ¥æœ€åä¸€æ¡æ¶ˆæ¯æ˜¯å¦æœ‰ tool_calls
        for msg in reversed(request.messages):
            if msg.tool_calls:
                for call in msg.tool_calls:
                    if call.get("function", {}).get("name") == "simulate_tx":
                        args = json.loads(call["function"]["arguments"])
                        return args.get("user_intent", ""), args

        # å°è¯•ä»æœ€åä¸€æ¡æ¶ˆæ¯è§£æ JSON
        last_message = request.messages[-1]
        try:
            data = json.loads(last_message.content)
            if "tx_from" in data and "tx_to" in data:
                return data.get("user_intent", ""), data
        except json.JSONDecodeError:
            pass

        # é»˜è®¤è¿”å›ç¤ºä¾‹
        return "è¯·å®¡è®¡æ­¤äº¤æ˜“", {
            "chain_id": 1,
            "tx_from": "0x" + "0" * 40,
            "tx_to": "0x" + "0" * 40,
            "tx_value": "0",
            "tx_data": "0x",
        }

    async def _run_simulation(
        self,
        request: SimulationRequest,
    ) -> SimulationResult:
        """
        è¿è¡Œäº¤æ˜“æ¨¡æ‹Ÿ

        MVP é˜¶æ®µï¼šè¿”å› Mock ç»“æœ
        ç”Ÿäº§ç¯å¢ƒï¼šä½¿ç”¨çœŸå®çš„ AnvilScreener æˆ– ROMA Pipeline
        """
        return SimulationResult(
            chain_id=request.chain_id,
            block_number=19_000_000,
            tx_from=request.tx_from,
            tx_to=request.tx_to,
            tx_value=request.tx_value,
            tx_data=request.tx_data,
            success=True,
            gas_used=150_000,
            asset_changes=[],
        )

    def _format_response_message(
        self,
        analysis: Any,
        result: SimulationResult,
    ) -> str:
        """æ ¼å¼åŒ–å“åº”æ¶ˆæ¯"""
        risk_emoji = {
            "SAFE": "âœ…",
            "WARNING": "âš ï¸",
            "CRITICAL": "ğŸš¨",
        }

        emoji = risk_emoji.get(analysis.risk_level.value, "")
        lines = [
            f"{emoji} **å®‰å…¨å®¡è®¡ç»“æœ**: {analysis.risk_level.value}",
            f"**ç½®ä¿¡åº¦**: {analysis.confidence:.0%}",
            "",
            f"**æ‘˜è¦**: {analysis.summary}",
        ]

        if analysis.anomalies:
            lines.extend(["", "**æ£€æµ‹åˆ°çš„é—®é¢˜**:"])
            lines.extend(f"- {a}" for a in analysis.anomalies)

        if analysis.recommendations:
            lines.extend(["", "**å»ºè®®**:"])
            lines.extend(f"- {r}" for r in analysis.recommendations)

        return "\n".join(lines)


# =============================================================================
# Helper Functions
# =============================================================================

def create_chat_completion_response(
    model: str,
    content: str,
    tool_calls: Optional[List[Dict[str, Any]]] = None,
    metadata: Optional[Dict[str, Any]] = None,
) -> ChatCompletionResponse:
    """åˆ›å»º Chat Completion å“åº”çš„ä¾¿æ·å‡½æ•°"""
    return ChatCompletionResponse(
        id=f"chatcmpl-{uuid.uuid4().hex[:28]}",
        created=int(time.time()),
        model=model,
        choices=[{
            "index": 0,
            "message": {
                "role": "assistant",
                "content": content,
                **({"tool_calls": tool_calls} if tool_calls else {}),
            },
            "finish_reason": "tool_calls" if tool_calls else "stop",
        }],
        usage=Usage(
            prompt_tokens=0,
            completion_tokens=0,
            total_tokens=0,
        ),
        metadata=metadata,
    )
