"""
OpenAI Chat Completion Compatible API

å®ç°ç¬¦åˆ OpenAI API æ ‡å‡†çš„æ¥å£ï¼Œä½¿ SSSEA èƒ½è¢«å…¶ä»– Agent é€šè¿‡æ ‡å‡† SDK è°ƒç”¨ã€‚
åŸºäº ROMA Pipeline è¿›è¡Œå®Œæ•´çš„é€’å½’æ¨ç†åˆ†æã€‚
"""

import json
import logging
import time
import uuid
from datetime import datetime, timezone
from typing import Any, Dict, List, Optional

from fastapi import HTTPException
from pydantic import BaseModel, Field

from ..attestation.mock_quote import generate_attestation_metadata
from ..config import get_settings


logger = logging.getLogger(__name__)


# =============================================================================
# OpenAI API Request/Response Models
# =============================================================================


class ChatMessage(BaseModel):
    """Chat æ¶ˆæ¯"""
    role: str
    content: str
    name: Optional[str] = None
    tool_calls: Optional[List[Any]] = None


class ToolFunction(BaseModel):
    """Tool å‡½æ•°å®šä¹‰"""
    name: str
    arguments: str  # JSON string


class ToolCall(BaseModel):
    """Tool è°ƒç”¨"""
    id: str
    type: str = "function"
    function: ToolFunction


class Tool(BaseModel):
    """Tool å®šä¹‰"""
    type: str = "function"
    function: Dict[str, Any]


class ToolChoice(BaseModel):
    """Tool é€‰æ‹©"""
    type: str = "function"
    function: Dict[str, str]


class ChatCompletionRequest(BaseModel):
    """Chat Completion è¯·æ±‚"""
    model: str
    messages: List[ChatMessage]
    tools: Optional[List[Tool]] = None
    tool_choice: Optional[str | ToolChoice] = "auto"
    temperature: Optional[float] = 0.7
    max_tokens: Optional[int] = None
    stream: Optional[bool] = False


class Usage(BaseModel):
    """Token ä½¿ç”¨ç»Ÿè®¡"""
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int


class ChatCompletionResponse(BaseModel):
    """Chat Completion å“åº”"""
    id: str
    object: str = "chat.completion"
    created: int
    model: str
    choices: List[Dict[str, Any]]
    usage: Usage
    system_fingerprint: Optional[str] = None

    # SSSEA æ‰©å±•å­—æ®µ
    metadata: Optional[Dict[str, Any]] = None


# =============================================================================
# SSSEA Tool Definitions
# =============================================================================

SIMULATE_TX_TOOL = {
    "type": "function",
    "function": {
        "name": "simulate_tx",
        "description": (
            "åœ¨ TEE éš”ç¦»æ²™ç›’ä¸­æ¨¡æ‹Ÿ Web3 äº¤æ˜“æ‰§è¡Œï¼Œå¹¶è¿›è¡Œæ„å›¾å¯¹é½å®¡è®¡ã€‚"
            "è¿”å›è¯¦ç»†çš„èµ„äº§å˜åŠ¨ã€é£é™©è¯„çº§å’Œ OML è¯æ˜ã€‚"
            "åŸºäºROMAæ¡†æ¶è¿›è¡Œé€’å½’æ¨ç†åˆ†æã€‚"
        ),
        "parameters": {
            "type": "object",
            "properties": {
                "user_intent": {
                    "type": "string",
                    "description": "ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€æ„å›¾ï¼Œå¦‚ 'Swap 1 ETH to USDC, slippage 0.5%'",
                },
                "chain_id": {
                    "type": "integer",
                    "description": "é“¾ IDï¼Œé»˜è®¤ä¸ºä»¥å¤ªåŠä¸»ç½‘ (1)",
                    "default": 1,
                },
                "tx_from": {
                    "type": "string",
                    "description": "äº¤æ˜“å‘èµ·è€…åœ°å€",
                },
                "tx_to": {
                    "type": "string",
                    "description": "äº¤æ˜“ç›®æ ‡åœ°å€",
                },
                "tx_value": {
                    "type": "string",
                    "description": "äº¤æ˜“ valueï¼ˆwei æ ¼å¼ï¼‰",
                    "default": "0",
                },
                "tx_data": {
                    "type": "string",
                    "description": "äº¤æ˜“ calldata",
                    "default": "0x",
                },
            },
            "required": ["user_intent", "tx_from", "tx_to"],
        },
    },
}


# =============================================================================
# API Handler
# =============================================================================

class SSSEAHandler:
    """
    SSSEA API å¤„ç†å™¨

    ä½¿ç”¨ ROMA Pipeline è¿›è¡Œå®Œæ•´çš„é€’å½’æ¨ç†åˆ†æã€‚
    """

    def __init__(self, settings: Optional[Any] = None):
        self.settings = settings or get_settings()
        self._roma_pipeline = None
        self._initialize_pipeline()

    def _initialize_pipeline(self) -> None:
        """åˆå§‹åŒ– ROMA Pipeline"""
        try:
            from ..agents import SSSEAPipeline
            from ..config.roma_config import load_profile

            # æ ¹æ®ç¯å¢ƒåŠ è½½é…ç½®
            profile = "dev" if self.settings.api_reload else "prod"
            config = load_profile(profile)
            self._roma_pipeline = SSSEAPipeline(config)
            logger.info(f"ROMA Pipeline initialized with profile: {profile}")

        except ImportError as e:
            logger.error(f"Failed to import ROMA Pipeline: {e}")
            raise
        except Exception as e:
            logger.error(f"Failed to initialize ROMA Pipeline: {e}")
            raise

    async def handle_chat_completion(
        self,
        request: ChatCompletionRequest,
    ) -> ChatCompletionResponse:
        """
        å¤„ç† Chat Completion è¯·æ±‚

        Args:
            request: Chat Completion è¯·æ±‚

        Returns:
            ChatCompletionResponse: å“åº”
        """
        # æ£€æŸ¥æ˜¯å¦è¯·æ±‚äº† simulate_tx å·¥å…·
        if request.tools and any(
            t.function.get("name") == "simulate_tx"
            for t in request.tools
        ):
            return await self._handle_simulation(request)

        # è¿”å›æ™®é€šèŠå¤©å“åº”
        return await self._handle_chat(request)

    async def _handle_simulation(
        self,
        request: ChatCompletionRequest,
    ) -> ChatCompletionResponse:
        """å¤„ç†æ¨¡æ‹Ÿè¯·æ±‚"""
        # 1. æå–æ„å›¾å’Œäº¤æ˜“æ•°æ®
        intent, tx_params = self._extract_transaction_params(request)

        # 2. è¿è¡Œ ROMA Pipeline
        return await self._handle_with_pipeline(request, intent, tx_params)

    async def _handle_with_pipeline(
        self,
        request: ChatCompletionRequest,
        intent: str,
        tx_params: Dict[str, Any],
    ) -> ChatCompletionResponse:
        """ä½¿ç”¨ROMA Pipelineå¤„ç†è¯·æ±‚"""
        try:
            # è¿è¡ŒROMA Pipeline
            result = await self._roma_pipeline.run(
                user_intent=intent,
                tx_data=tx_params,
            )

            # æ„å»ºå“åº”
            return self._build_response(request, intent, tx_params, result)

        except Exception as e:
            logger.error(f"ROMA Pipelineæ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
            raise HTTPException(
                status_code=500,
                detail=f"æ¨¡æ‹Ÿæ‰§è¡Œå¤±è´¥: {str(e)}"
            )

    def _build_response(
        self,
        request: ChatCompletionRequest,
        intent: str,
        tx_params: Dict[str, Any],
        result: Dict[str, Any],
    ) -> ChatCompletionResponse:
        """æ„å»ºå“åº”"""
        verdict = result.get("verdict", {})
        tool_call_id = f"call_{uuid.uuid4().hex[:24]}"

        # æ ¼å¼åŒ–å“åº”æ¶ˆæ¯
        content = self._format_result_message(result)

        # ç”Ÿæˆè¯æ˜
        attestation = generate_attestation_metadata(
            simulation_result={
                "risk_level": verdict.get("risk_level", "UNKNOWN"),
                "confidence": verdict.get("confidence", 0.7),
            },
            model_name=request.model,
        )

        return ChatCompletionResponse(
            id=f"chatcmpl-{uuid.uuid4().hex[:28]}",
            created=int(time.time()),
            model=request.model,
            choices=[{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": content,
                    "tool_calls": [{
                        "id": tool_call_id,
                        "type": "function",
                        "function": {
                            "name": "simulate_tx",
                            "arguments": json.dumps(tx_params),
                        },
                    }],
                },
                "finish_reason": "tool_calls",
            }],
            usage=Usage(
                prompt_tokens=100,
                completion_tokens=len(content) // 4,
                total_tokens=100 + len(content) // 4,
            ),
            system_fingerprint=attestation["system_fingerprint"],
            metadata={
                "oml_attestation": attestation["oml_attestation"],
                "risk_level": verdict.get("risk_level", "UNKNOWN"),
                "risk_score": int(verdict.get("confidence", 0.7) * 100),
                "pipeline_used": True,
                "execution_steps": result.get("execution_details", {}).get("steps", []),
            },
        )

    def _format_result_message(self, result: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–ç»“æœæ¶ˆæ¯"""
        verdict = result.get("verdict", {})
        risk_level = verdict.get("risk_level", "UNKNOWN")

        risk_emoji = {
            "SAFE": "âœ…",
            "WARNING": "âš ï¸",
            "CRITICAL": "ğŸš¨",
        }

        emoji = risk_emoji.get(risk_level, "")
        lines = [
            f"{emoji} **å®‰å…¨å®¡è®¡ç»“æœ**: {risk_level}",
            f"**ç½®ä¿¡åº¦**: {verdict.get('confidence', 0.7):.0%}",
            "",
            f"**æ‘˜è¦**: {result.get('summary', '')}",
        ]

        findings = result.get("findings", [])
        if findings:
            lines.extend(["", "**æ£€æµ‹åˆ°çš„é—®é¢˜**:"])
            lines.extend(f"- {f}" for f in findings)

        recommendations = result.get("recommendations", [])
        if recommendations:
            lines.extend(["", "**å»ºè®®**:"])
            lines.extend(f"- {r}" for r in recommendations[:5])

        return "\n".join(lines)

    async def _handle_chat(
        self,
        request: ChatCompletionRequest,
    ) -> ChatCompletionResponse:
        """å¤„ç†æ™®é€šèŠå¤©è¯·æ±‚"""
        response = ChatCompletionResponse(
            id=f"chatcmpl-{uuid.uuid4().hex[:28]}",
            created=int(time.time()),
            model=request.model,
            choices=[{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": (
                        "æˆ‘æ˜¯ SSSEA å®‰å…¨å®¡è®¡ Agentï¼ŒåŸºäº ROMA æ¡†æ¶è¿›è¡Œé€’å½’æ¨ç†åˆ†æã€‚"
                        "è¯·ä½¿ç”¨ simulate_tx å·¥å…·æ¥å®¡è®¡ Web3 äº¤æ˜“ã€‚"
                    ),
                },
                "finish_reason": "stop",
            }],
            usage=Usage(
                prompt_tokens=10,
                completion_tokens=25,
                total_tokens=35,
            ),
            system_fingerprint=f"sssea-roma@{uuid.uuid4().hex[:8]}",
        )

        return response

    def _extract_transaction_params(
        self,
        request: ChatCompletionRequest,
    ) -> tuple[str, Dict[str, Any]]:
        """
        ä»è¯·æ±‚ä¸­æå–äº¤æ˜“å‚æ•°

        ä¼˜å…ˆçº§ï¼š
        1. tool_calls ä¸­çš„å‚æ•°
        2. ç”¨æˆ·æ¶ˆæ¯ä¸­çš„ JSON
        3. æ¶ˆæ¯æ–‡æœ¬è§£æ
        """
        # æ£€æŸ¥æœ€åä¸€æ¡æ¶ˆæ¯æ˜¯å¦æœ‰ tool_calls
        for msg in reversed(request.messages):
            if msg.tool_calls:
                for call in msg.tool_calls:
                    if call.get("function", {}).get("name") == "simulate_tx":
                        args = json.loads(call["function"]["arguments"])
                        return args.get("user_intent", ""), args

        # å°è¯•ä»æœ€åä¸€æ¡æ¶ˆæ¯è§£æ JSON
        last_message = request.messages[-1]
        try:
            data = json.loads(last_message.content)
            if "tx_from" in data and "tx_to" in data:
                return data.get("user_intent", ""), data
        except json.JSONDecodeError:
            pass

        # é»˜è®¤è¿”å›ç¤ºä¾‹
        return "è¯·å®¡è®¡æ­¤äº¤æ˜“", {
            "chain_id": 1,
            "tx_from": "0x" + "0" * 40,
            "tx_to": "0x" + "0" * 40,
            "tx_value": "0",
            "tx_data": "0x",
        }


# =============================================================================
# Helper Functions
# =============================================================================

def create_chat_completion_response(
    model: str,
    content: str,
    tool_calls: Optional[List[Dict[str, Any]]] = None,
    metadata: Optional[Dict[str, Any]] = None,
) -> ChatCompletionResponse:
    """åˆ›å»º Chat Completion å“åº”çš„ä¾¿æ·å‡½æ•°"""
    return ChatCompletionResponse(
        id=f"chatcmpl-{uuid.uuid4().hex[:28]}",
        created=int(time.time()),
        model=model,
        choices=[{
            "index": 0,
            "message": {
                "role": "assistant",
                "content": content,
                **({"tool_calls": tool_calls} if tool_calls else {}),
            },
            "finish_reason": "tool_calls" if tool_calls else "stop",
        }],
        usage=Usage(
            prompt_tokens=0,
            completion_tokens=0,
            total_tokens=0,
        ),
        metadata=metadata,
    )
