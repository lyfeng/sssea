"""
OpenAI Chat Completion Compatible API

å®ç°ç¬¦åˆ OpenAI API æ ‡å‡†çš„æ¥å£ï¼Œä½¿ SSSEA èƒ½è¢«å…¶ä»– Agent é€šè¿‡æ ‡å‡† SDK è°ƒç”¨ã€‚
"""

import json
import logging
import time
import uuid
from datetime import datetime, timezone
from typing import Any, Dict, List, Optional

from fastapi import HTTPException
from pydantic import BaseModel, Field

from ..simulation.models import SimulationRequest, SimulationResult
from ..simulation.anvil_screener import AnvilScreener
from ..reasoning.intent_analyzer import MockIntentAnalyzer
from ..attestation.mock_quote import generate_attestation_metadata
from ..config import get_settings


logger = logging.getLogger(__name__)


# =============================================================================
# OpenAI API Request/Response Models
# =============================================================================


class ChatMessage(BaseModel):
    """Chat æ¶ˆæ¯"""
    role: str
    content: str
    name: Optional[str] = None
    tool_calls: Optional[List[Any]] = None


class ToolFunction(BaseModel):
    """Tool å‡½æ•°å®šä¹‰"""
    name: str
    arguments: str  # JSON string


class ToolCall(BaseModel):
    """Tool è°ƒç”¨"""
    id: str
    type: str = "function"
    function: ToolFunction


class Tool(BaseModel):
    """Tool å®šä¹‰"""
    type: str = "function"
    function: Dict[str, Any]


class ToolChoice(BaseModel):
    """Tool é€‰æ‹©"""
    type: str = "function"
    function: Dict[str, str]


class ChatCompletionRequest(BaseModel):
    """Chat Completion è¯·æ±‚"""
    model: str
    messages: List[ChatMessage]
    tools: Optional[List[Tool]] = None
    tool_choice: Optional[str | ToolChoice] = "auto"
    temperature: Optional[float] = 0.7
    max_tokens: Optional[int] = None
    stream: Optional[bool] = False


class Usage(BaseModel):
    """Token ä½¿ç”¨ç»Ÿè®¡"""
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int


class ChatCompletionResponse(BaseModel):
    """Chat Completion å“åº”"""
    id: str
    object: str = "chat.completion"
    created: int
    model: str
    choices: List[Dict[str, Any]]
    usage: Usage
    system_fingerprint: Optional[str] = None

    # SSSEA æ‰©å±•å­—æ®µ
    metadata: Optional[Dict[str, Any]] = None


class SimulationToolArgs(BaseModel):
    """simulate_tx å·¥å…·å‚æ•°"""
    user_intent: str
    chain_id: int = 1
    tx_from: str
    tx_to: str
    tx_value: str = "0"
    tx_data: str = "0x"


# =============================================================================
# SSSEA Tool Definitions
# =============================================================================

SIMULATE_TX_TOOL = {
    "type": "function",
    "function": {
        "name": "simulate_tx",
        "description": (
            "åœ¨ TEE éš”ç¦»æ²™ç›’ä¸­æ¨¡æ‹Ÿ Web3 äº¤æ˜“æ‰§è¡Œï¼Œå¹¶è¿›è¡Œæ„å›¾å¯¹é½å®¡è®¡ã€‚"
            "è¿”å›è¯¦ç»†çš„èµ„äº§å˜åŠ¨ã€é£é™©è¯„çº§å’Œ OML è¯æ˜ã€‚"
        ),
        "parameters": {
            "type": "object",
            "properties": {
                "user_intent": {
                    "type": "string",
                    "description": "ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€æ„å›¾ï¼Œå¦‚ 'Swap 1 ETH to USDC, slippage 0.5%'",
                },
                "chain_id": {
                    "type": "integer",
                    "description": "é“¾ IDï¼Œé»˜è®¤ä¸ºä»¥å¤ªåŠä¸»ç½‘ (1)",
                    "default": 1,
                },
                "tx_from": {
                    "type": "string",
                    "description": "äº¤æ˜“å‘èµ·è€…åœ°å€",
                },
                "tx_to": {
                    "type": "string",
                    "description": "äº¤æ˜“ç›®æ ‡åœ°å€",
                },
                "tx_value": {
                    "type": "string",
                    "description": "äº¤æ˜“ valueï¼ˆwei æ ¼å¼ï¼‰",
                    "default": "0",
                },
                "tx_data": {
                    "type": "string",
                    "description": "äº¤æ˜“ calldata",
                    "default": "0x",
                },
            },
            "required": ["user_intent", "tx_from", "tx_to"],
        },
    },
}


# =============================================================================
# API Handler
# =============================================================================

class SSSEAHandler:
    """
    SSSEA API å¤„ç†å™¨

    å¤„ç† OpenAI å…¼å®¹çš„ Chat Completion è¯·æ±‚ã€‚
    """

    def __init__(self, settings: Optional[Any] = None):
        self.settings = settings or get_settings()
        self.analyzer = MockIntentAnalyzer()
        self._screener: Optional[AnvilScreener] = None

    async def handle_chat_completion(
        self,
        request: ChatCompletionRequest,
    ) -> ChatCompletionResponse:
        """
        å¤„ç† Chat Completion è¯·æ±‚

        Args:
            request: Chat Completion è¯·æ±‚

        Returns:
            ChatCompletionResponse: å“åº”
        """
        # æ£€æŸ¥æ˜¯å¦è¯·æ±‚äº† simulate_tx å·¥å…·
        if request.tools and any(
            t.function.get("name") == "simulate_tx"
            for t in request.tools
        ):
            return await self._handle_simulation(request)

        # å¦‚æœæ²¡æœ‰è¯·æ±‚å·¥å…·ï¼Œè¿”å›æ™®é€šèŠå¤©å“åº”
        return await self._handle_chat(request)

    async def _handle_simulation(
        self,
        request: ChatCompletionRequest,
    ) -> ChatCompletionResponse:
        """
        å¤„ç†æ¨¡æ‹Ÿè¯·æ±‚

        1. è§£æç”¨æˆ·æ¶ˆæ¯ä¸­çš„æ„å›¾å’Œäº¤æ˜“æ•°æ®
        2. è°ƒç”¨ AnvilScreener æ‰§è¡Œæ¨¡æ‹Ÿ
        3. è°ƒç”¨ IntentAnalyzer è¿›è¡Œæ„å›¾å®¡è®¡
        4. è¿”å›å¸¦ OML è¯æ˜çš„å“åº”
        """
        # 1. æå–æ„å›¾å’Œäº¤æ˜“æ•°æ®
        intent, tx_params = self._extract_transaction_params(request)

        # 2. æ„å»ºæ¨¡æ‹Ÿè¯·æ±‚
        sim_request = SimulationRequest(
            user_intent=intent,
            chain_id=tx_params.get("chain_id", 1),
            tx_from=tx_params["tx_from"],
            tx_to=tx_params["tx_to"],
            tx_value=tx_params.get("tx_value", "0"),
            tx_data=tx_params.get("tx_data", "0x"),
        )

        # 3. æ‰§è¡Œæ¨¡æ‹Ÿï¼ˆMVP é˜¶æ®µä½¿ç”¨ Mock ç»“æœï¼‰
        sim_result = await self._run_simulation(sim_request)

        # 4. æ„å›¾åˆ†æ
        analysis = await self.analyzer.analyze(sim_request, sim_result)

        # 5. ç”Ÿæˆ OML è¯æ˜
        attestation = generate_attestation_metadata(
            simulation_result={
                "risk_level": analysis.risk_level.value,
                "confidence": analysis.confidence,
                "anomalies": analysis.anomalies,
            },
            model_name=request.model,
        )

        # 6. æ„å»º Tool Call å“åº”
        tool_call_id = f"call_{uuid.uuid4().hex[:24]}"
        result_data = {
            "verdict": analysis.risk_level.value,
            "confidence": analysis.confidence,
            "summary": analysis.summary,
            "analysis": analysis.analysis,
            "asset_changes": [
                {
                    "token": c.token_symbol,
                    "amount": c.change_amount,
                }
                for c in sim_result.asset_changes
            ],
            "anomalies": analysis.anomalies,
            "recommendations": analysis.recommendations,
            "gas_used": sim_result.gas_used,
        }

        response = ChatCompletionResponse(
            id=f"chatcmpl-{uuid.uuid4().hex[:28]}",
            created=int(time.time()),
            model=request.model,
            choices=[{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": self._format_response_message(analysis, sim_result),
                    "tool_calls": [{
                        "id": tool_call_id,
                        "type": "function",
                        "function": {
                            "name": "simulate_tx",
                            "arguments": json.dumps(tx_params),
                        },
                    }],
                },
                "finish_reason": "tool_calls",
            }],
            usage=Usage(
                prompt_tokens=100,  # Mock
                completion_tokens=len(analysis.analysis) // 4,
                total_tokens=100 + len(analysis.analysis) // 4,
            ),
            system_fingerprint=attestation["system_fingerprint"],
            metadata={
                "oml_attestation": attestation["oml_attestation"],
                "risk_level": analysis.risk_level.value,
                "risk_score": int(analysis.confidence * 100),
                "asset_impact": {
                    c.token_symbol: c.change_amount
                    for c in sim_result.asset_changes
                },
            },
        )

        return response

    async def _handle_chat(
        self,
        request: ChatCompletionRequest,
    ) -> ChatCompletionResponse:
        """å¤„ç†æ™®é€šèŠå¤©è¯·æ±‚"""
        last_message = request.messages[-1].content if request.messages else ""

        response = ChatCompletionResponse(
            id=f"chatcmpl-{uuid.uuid4().hex[:28]}",
            created=int(time.time()),
            model=request.model,
            choices=[{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": (
                        "æˆ‘æ˜¯ SSSEA å®‰å…¨å®¡è®¡ Agentã€‚"
                        "è¯·ä½¿ç”¨ simulate_tx å·¥å…·æ¥å®¡è®¡ Web3 äº¤æ˜“ã€‚"
                    ),
                },
                "finish_reason": "stop",
            }],
            usage=Usage(
                prompt_tokens=10,
                completion_tokens=20,
                total_tokens=30,
            ),
            system_fingerprint=f"sssea@mock_{uuid.uuid4().hex[:8]}",
        )

        return response

    def _extract_transaction_params(
        self,
        request: ChatCompletionRequest,
    ) -> tuple[str, Dict[str, Any]]:
        """
        ä»è¯·æ±‚ä¸­æå–äº¤æ˜“å‚æ•°

        ä¼˜å…ˆçº§ï¼š
        1. tool_calls ä¸­çš„å‚æ•°
        2. ç”¨æˆ·æ¶ˆæ¯ä¸­çš„ JSON
        3. æ¶ˆæ¯æ–‡æœ¬è§£æ
        """
        # æ£€æŸ¥æœ€åä¸€æ¡æ¶ˆæ¯æ˜¯å¦æœ‰ tool_calls
        for msg in reversed(request.messages):
            if msg.tool_calls:
                for call in msg.tool_calls:
                    if call.get("function", {}).get("name") == "simulate_tx":
                        args = json.loads(call["function"]["arguments"])
                        return args.get("user_intent", ""), args

        # å°è¯•ä»æœ€åä¸€æ¡æ¶ˆæ¯è§£æ JSON
        last_message = request.messages[-1]
        try:
            data = json.loads(last_message.content)
            if "tx_from" in data and "tx_to" in data:
                return data.get("user_intent", ""), data
        except json.JSONDecodeError:
            pass

        # é»˜è®¤è¿”å›ç¤ºä¾‹
        return "è¯·å®¡è®¡æ­¤äº¤æ˜“", {
            "chain_id": 1,
            "tx_from": "0x" + "0" * 40,
            "tx_to": "0x" + "0" * 40,
            "tx_value": "0",
            "tx_data": "0x",
        }

    async def _run_simulation(
        self,
        request: SimulationRequest,
    ) -> SimulationResult:
        """
        è¿è¡Œäº¤æ˜“æ¨¡æ‹Ÿ

        MVP é˜¶æ®µï¼šè¿”å› Mock ç»“æœ
        ç”Ÿäº§ç¯å¢ƒï¼šä½¿ç”¨çœŸå®çš„ AnvilScreener
        """
        # MVP é˜¶æ®µè¿”å› Mock ç»“æœ
        return SimulationResult(
            chain_id=request.chain_id,
            block_number=19_000_000,
            tx_from=request.tx_from,
            tx_to=request.tx_to,
            tx_value=request.tx_value,
            tx_data=request.tx_data,
            success=True,
            gas_used=150_000,
            asset_changes=[
                # Mock: å‡è®¾æ˜¯ä¸€ä¸ªæˆåŠŸçš„ swap
                # å®é™…ç¯å¢ƒä¼šä» Anvil è·å–çœŸå®æ•°æ®
            ],
        )

    def _format_response_message(
        self,
        analysis: Any,
        result: SimulationResult,
    ) -> str:
        """æ ¼å¼åŒ–å“åº”æ¶ˆæ¯"""
        risk_emoji = {
            "SAFE": "âœ…",
            "WARNING": "âš ï¸",
            "CRITICAL": "ğŸš¨",
        }

        emoji = risk_emoji.get(analysis.risk_level.value, "")
        lines = [
            f"{emoji} **å®‰å…¨å®¡è®¡ç»“æœ**: {analysis.risk_level.value}",
            f"**ç½®ä¿¡åº¦**: {analysis.confidence:.0%}",
            "",
            f"**æ‘˜è¦**: {analysis.summary}",
        ]

        if analysis.anomalies:
            lines.extend(["", "**æ£€æµ‹åˆ°çš„é—®é¢˜**:"])
            lines.extend(f"- {a}" for a in analysis.anomalies)

        if analysis.recommendations:
            lines.extend(["", "**å»ºè®®**:"])
            lines.extend(f"- {r}" for r in analysis.recommendations)

        return "\n".join(lines)


# =============================================================================
# Helper Functions
# =============================================================================

def create_chat_completion_response(
    model: str,
    content: str,
    tool_calls: Optional[List[Dict[str, Any]]] = None,
    metadata: Optional[Dict[str, Any]] = None,
) -> ChatCompletionResponse:
    """åˆ›å»º Chat Completion å“åº”çš„ä¾¿æ·å‡½æ•°"""
    return ChatCompletionResponse(
        id=f"chatcmpl-{uuid.uuid4().hex[:28]}",
        created=int(time.time()),
        model=model,
        choices=[{
            "index": 0,
            "message": {
                "role": "assistant",
                "content": content,
                **({"tool_calls": tool_calls} if tool_calls else {}),
            },
            "finish_reason": "tool_calls" if tool_calls else "stop",
        }],
        usage=Usage(
            prompt_tokens=0,
            completion_tokens=0,
            total_tokens=0,
        ),
        metadata=metadata,
    )
